{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 512), (10, 512), (10, 512))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "sequence_length, d_k, d_v = 10, 512, 512\n",
    "# random sequences\n",
    "q = np.random.randn(sequence_length, d_k)\n",
    "k = np.random.randn(sequence_length, d_k)\n",
    "v = np.random.randn(sequence_length, d_v)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dot product of Query and Key to get the similarity between them, which is attention score\n",
    "\n",
    "> Divide by the degree of Key to reduce variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 10),\n",
       " array([[-0.24377662,  1.02113081,  1.3023725 ,  1.27063396,  0.61714306,\n",
       "         -0.24391184, -1.77506907,  0.10631629,  1.42332514,  1.72122403],\n",
       "        [-1.712277  , -0.34658796,  0.3629974 , -0.76977711,  1.08587583,\n",
       "          0.16609727,  0.27010721,  0.15325385,  0.18152852,  0.49246579],\n",
       "        [ 1.05832548,  1.98228014,  2.18850259,  0.82108503,  0.42838988,\n",
       "          1.8003839 , -0.39505285, -0.02929142, -0.69937748, -0.23668556],\n",
       "        [ 1.09747933,  0.32267196, -0.20756995, -0.85043791, -0.20697511,\n",
       "         -0.15425011,  0.59800415, -0.13253693,  1.38956401,  0.55788049],\n",
       "        [ 1.442453  , -0.17433992,  1.44209349, -0.37922308,  0.06991483,\n",
       "         -0.76360436,  1.27243722,  0.73817403, -1.301752  , -0.46046847],\n",
       "        [ 1.9083543 , -1.33721653,  0.99054056,  0.1473052 , -0.4756667 ,\n",
       "         -0.67855265,  1.67230011,  1.19530252, -1.51350064,  0.12218795],\n",
       "        [ 1.51843864,  0.3968648 ,  0.73078071,  0.43601877, -2.06270872,\n",
       "         -0.2010952 , -0.3849416 , -0.36601384,  0.22421066,  0.10103515],\n",
       "        [ 1.18249188,  0.93866833,  0.77501054,  0.04092458, -0.05374637,\n",
       "         -0.94093601, -0.71874403, -1.26376346,  1.06500405, -0.55640732],\n",
       "        [-0.41315987,  0.60340905,  1.45690012, -0.99235632,  1.23683168,\n",
       "         -0.9963281 , -0.28592463,  1.71987027,  0.26580614,  1.15352023],\n",
       "        [-0.92585863, -0.729652  , -1.68159077, -0.11462305, -0.41757294,\n",
       "          0.25874514, -0.11306163, -0.71986568, -0.10551769,  0.23315013]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T)\n",
    "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "\n",
    "scaled.shape, scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Masking to hide the future sequences for the decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 10),\n",
       " array([[  0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [  0.,   0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [  0.,   0.,   0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [  0.,   0.,   0.,   0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [  0.,   0.,   0.,   0.,   0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0., -inf, -inf, -inf, -inf],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0., -inf, -inf, -inf],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., -inf, -inf],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., -inf],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones((sequence_length, sequence_length)))\n",
    "mask[ mask == 0 ] = -np.infty\n",
    "mask[ mask == 1 ] = 0\n",
    "\n",
    "mask.shape, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Transform to probability matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 10),\n",
       " array([[1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.20331724, 0.79668276, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.15116157, 0.38081127, 0.46802717, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.53346922, 0.24581863, 0.14465507, 0.07605708, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.38264445, 0.07596808, 0.38250691, 0.06189443, 0.09698614,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.5625456 , 0.02190908, 0.2246758 , 0.0966816 , 0.05185499,\n",
       "         0.04233293, 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.40395754, 0.13159591, 0.18376391, 0.13685061, 0.01124769,\n",
       "         0.07236885, 0.06021548, 0.        , 0.        , 0.        ],\n",
       "        [0.29288024, 0.22950854, 0.19486021, 0.09352198, 0.08507435,\n",
       "         0.03503459, 0.04375159, 0.02536851, 0.        , 0.        ],\n",
       "        [0.03555465, 0.09826224, 0.23070293, 0.01992299, 0.18513076,\n",
       "         0.01984402, 0.04037885, 0.30009557, 0.070108  , 0.        ],\n",
       "        [0.05316333, 0.06468799, 0.02496904, 0.11965403, 0.08838078,\n",
       "         0.17381172, 0.119841  , 0.06532415, 0.12074849, 0.16941946]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "attention = softmax(scaled + mask)\n",
    "\n",
    "attention.shape, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Apply to original Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 512),\n",
       " array([[-0.77619716,  0.78746114, -1.3483487 , ...,  0.72068791,\n",
       "         -0.13545548, -0.05036879],\n",
       "        [-0.62739304, -0.32487966,  0.68112591, ..., -0.38931659,\n",
       "          0.11685185,  0.04739216],\n",
       "        [-0.29292767, -0.15508948, -0.22815548, ...,  0.15647082,\n",
       "          0.77177643, -0.12194113],\n",
       "        ...,\n",
       "        [-0.50205301,  0.18204522, -0.29486757, ...,  0.14680658,\n",
       "          0.25700999,  0.13210688],\n",
       "        [-0.58436974, -0.28711701, -0.10361234, ..., -0.14411789,\n",
       "          0.44227375,  0.76628905],\n",
       "        [-0.47548399, -0.01235199, -0.00600266, ..., -0.43051166,\n",
       "         -0.18088839,  0.12450246]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v.shape, new_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  return (np.exp(x).T / np.sum(np.exp(x), axis = -1) ).T\n",
    "\n",
    "# Numpy edition\n",
    "def scaled_dot_product_np(q, k, v, masking=None):\n",
    "  d_k = q.shape[-1]\n",
    "  scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "  if masking is not None:\n",
    "    scaled += masking\n",
    "  attention = softmax(scaled)\n",
    "  new_v = np.matmul(attention, v)\n",
    "  return new_v, attention\n",
    "\n",
    "def create_mask_np(sequence_length=512):\n",
    "  mask = np.tril(np.ones((sequence_length, sequence_length)))\n",
    "  mask[ mask == 0 ] = -np.infty\n",
    "  mask[ mask == 1 ] = 0\n",
    "\n",
    "# Torch edition\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "  d_k = q.size()[-1]\n",
    "  scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "  if mask is not None:\n",
    "      scaled += mask\n",
    "  attention = F.softmax(scaled, dim=-1)\n",
    "  values = torch.matmul(attention, v)\n",
    "  return values, attention\n",
    "\n",
    "def create_mask(batch_size=1, num_heads=8, sequence_length=512):\n",
    "  mask = torch.full([batch_size, num_heads, sequence_length, sequence_length] , float('-inf'))\n",
    "  mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "# encoder_value, encoder_attention = scaled_dot_product_np(q, k, v)\n",
    "# decoder_value, decoder_attention = scaled_dot_product_np(q, k, v, mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
